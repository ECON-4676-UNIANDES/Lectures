\documentclass[
  shownotes,
  xcolor={svgnames},
  hyperref={colorlinks,citecolor=DarkBlue,linkcolor=DarkRed,urlcolor=DarkBlue}
  ]{beamer}
\usepackage{animate}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{pifont}
\usepackage{mathpazo}
%\usepackage{xcolor}
\usepackage{multimedia}
\usepackage{fancybox}
\usepackage[para]{threeparttable}
\usepackage{multirow}
\setcounter{MaxMatrixCols}{30}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{lscape}
\usepackage[compatibility=false,font=small]{caption}
\usepackage{booktabs}
\usepackage{ragged2e}
\usepackage{chronosys}
\usepackage{appendixnumberbeamer}
\usepackage{animate}
\setbeamertemplate{caption}[numbered]
\usepackage{color}
%\usepackage{times}
\usepackage{tikz}
\usepackage{comment} %to comment
%% BibTeX settings
\usepackage{natbib}
\bibliographystyle{apalike}
\bibpunct{(}{)}{,}{a}{,}{,}
\setbeamertemplate{bibliography item}{[\theenumiv]}

% Defines columns for bespoke tables
\usepackage{array}
\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}


\usepackage{xfrac}


\usepackage{multicol}
\setlength{\columnsep}{0.5cm}

% Theme and colors
\usetheme{Boadilla}

% I use steel blue and a custom color palette. This defines it.
\definecolor{andesred}{HTML}{af2433}

% Other options
\providecommand{\U}[1]{\protect\rule{.1in}{.1in}}
\usefonttheme{serif}
\setbeamertemplate{itemize items}[default]
\setbeamertemplate{enumerate items}[square]
\setbeamertemplate{section in toc}[circle]

\makeatletter

\definecolor{mybackground}{HTML}{82CAFA}
\definecolor{myforeground}{HTML}{0000A0}

\setbeamercolor{normal text}{fg=black,bg=white}
\setbeamercolor{alerted text}{fg=red}
\setbeamercolor{example text}{fg=black}

\setbeamercolor{background canvas}{fg=myforeground, bg=white}
\setbeamercolor{background}{fg=myforeground, bg=mybackground}

\setbeamercolor{palette primary}{fg=black, bg=gray!30!white}
\setbeamercolor{palette secondary}{fg=black, bg=gray!20!white}
\setbeamercolor{palette tertiary}{fg=white, bg=andesred}

\setbeamercolor{frametitle}{fg=andesred}
\setbeamercolor{title}{fg=andesred}
\setbeamercolor{block title}{fg=andesred}
\setbeamercolor{itemize item}{fg=andesred}
\setbeamercolor{itemize subitem}{fg=andesred}
\setbeamercolor{itemize subsubitem}{fg=andesred}
\setbeamercolor{enumerate item}{fg=andesred}
\setbeamercolor{item projected}{bg=gray!30!white,fg=andesred}
\setbeamercolor{enumerate subitem}{fg=andesred}
\setbeamercolor{section number projected}{bg=gray!30!white,fg=andesred}
\setbeamercolor{section in toc}{fg=andesred}
\setbeamercolor{caption name}{fg=andesred}
\setbeamercolor{button}{bg=gray!30!white,fg=andesred}


\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx}
\makeatletter

\usepackage{tikz}
% Tikz settings optimized for causal graphs.
\usetikzlibrary{shapes,decorations,arrows,calc,arrows.meta,fit,positioning}
\tikzset{
    -Latex,auto,node distance =1 cm and 1 cm,semithick,
    state/.style ={ellipse, draw, minimum width = 0.7 cm},
    point/.style = {circle, draw, inner sep=0.04cm,fill,node contents={}},
    bidirected/.style={Latex-Latex,dashed},
    el/.style = {inner sep=2pt, align=left, sloped}
}


\makeatother






%%%%%%%%%%%%%%% BEGINS DOCUMENT %%%%%%%%%%%%%%%%%%

\begin{document}

\title[Lecture 7]{Lecture 7: \\ Estimation Methods \\ Maximum Likelihood \& Bayesian Estimation}
\subtitle{Big Data and Machine Learning for Applied Economics \\ Econ 4676}
\date{\today}

\author[Sarmiento-Barbieri]{Ignacio Sarmiento-Barbieri}
\institute[Uniandes]{Universidad de los Andes}


\begin{frame}[noframenumbering]
\maketitle
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%----------------------------------------------------------------------%
\begin{frame}
\frametitle{Recap}

\begin{itemize} 
      
    \item Computation
    \bigskip
    \item QR decomposition
    \bigskip
    \item MapReduce and Spark
    \bigskip
    \item Demo \texttt{Scraping}
    \bigskip
    \item Message: web scraping involves as much art as it does science
  \bigskip  
\end{itemize}
\end{frame}

%----------------------------------------------------------------------%

\begin{frame}
\frametitle{Agenda}

\tableofcontents


\end{frame}



%----------------------------------------------------------------------%
\section{Motivation}
%----------------------------------------------------------------------%


\begin{frame}[fragile]
\frametitle{Motivation}

\begin{itemize}
    \item Maximum Likelihood is, by far, the most popular technique for deriving estimators
    \bigskip 
    \item Developed by Ronald A. Fisher (1890-1962)
    \bigskip
    \item ``If Fisher had lived in the era of ``apps,'' maximum likelihood estimation might have made him a billionaire'' (Efron and Tibshiriani, 2016)
    \bigskip
    \item  Why? MLE gives ``automatically''
    \begin{itemize}
      \item Unbiasedness 
      \medskip
      \item Minimum variance
    \end{itemize} 
\end{itemize}
 


 \end{frame}
%----------------------------------------------------------------------%
\section{Maximum Likelihood Estimation}
\begin{frame}[fragile]
\frametitle{Maximum Likelihood Estimation}

Let $X_1,\dots,X_n\sim_{iid}f(x|\theta)$, the likelihood function is defined by

\begin{align}\label{eq:1}
L(\theta|x)=\Pi_{i=1}^n f(x_i|\theta)
\end{align}

A maximum likelihood estimator of the parameter $\theta$:

\begin{align}
\hat \theta^{MLE}=\underset{\theta \in \Theta}{argmax}\, L(\theta,x)
\end{align}

\begin{itemize}
  \item Intuitively, MLE is a reasonable choice for an estimator.
  \item MLE is the parameter point for which the observed sample is most likely
  \item {\it It is kind of a ‘reverse engineering’ process:  to generate random numbers for a certain distribution you first set parameter values and then get realizations. This is doing the reverse process:  first set the realizations and try to get the parameters that are ‘most likely’ to have generated them}
\end{itemize}


 \end{frame}
%----------------------------------------------------------------------%

\begin{frame}[fragile]
\frametitle{Maximum Likelihood Estimation}
Note that maximizing \eqref{eq:1} is the same as maximizing

\begin{align}\label{eq:2}
l(\theta|x)=\ln L(\theta|x)=\sum_{i=1}^n l_i(x|\theta)
\end{align}

\bigskip
Advantages of \eqref{eq:2}
\begin{itemize}
\item It is easy to see that the {\bf contribution} of observation $i$ to the likelihood is given by $l_i(x|\theta)=\ln f(x_i|\theta)$
\item Eq. \eqref{eq:1} is also prone to underflow; can be very large or very small number that it cannot easily be represented in a computer. 
\end{itemize}

\end{frame}

%----------------------------------------------------------------------% 

\begin{frame}[fragile]
\frametitle{Maximum Likelihood Estimation}
If the likelihood function is differentiable (in $\theta$) a possible candidate for the MLE are the values of $\theta$ that solve

\begin{align}
  \frac{\partial L(\theta|x)}{\partial \theta} = 0
\end{align}

\begin{itemize}
\item These are  only {\it possible candidates}, this is a necessary condition for a max
\item Need to check SOC
\end{itemize}

\end{frame}
%----------------------------------------------------------------------%

\begin{frame}[fragile]
\frametitle{Maximum Likelihood Estimation}

 Let $X_1,\dots,X_n \sim N(\mu,1)$. We want to estimate $\theta = \mu$

Here

\begin{align}
L(\theta |x)=\frac{1}{(\sqrt{2\pi})^{n}}e^{-\frac{1}{2}\sum_{i=1}^{n}(x_i-\mu)^2}
\end{align}


taking logs 

\begin{align}
l\left(\theta |x\right)=-\frac{n}{2}log\left(2\pi\right)-\frac{1}{2}\sum_{i=1}^{n}(x_i-\mu)^2
\end{align}

FOC

\begin{align}
\frac{\partial l\left(\theta |x\right)}{\partial\mu}=0
\end{align}


\end{frame}
%----------------------------------------------------------------------%

\begin{frame}[fragile]
\frametitle{Maximum Likelihood Estimation}

\begin{align}
\frac{\partial l\left(\theta |x\right)}{\partial\mu}=2\frac{\sum_{i=1}^{n}\left(x_{i}-\mu\right)}{2}=0
\end{align}


\begin{align}
\sum_{i=1}^{n}\left(x_{i}-\hat{\mu}\right)=0
\end{align}

then

\begin{align}
\hat{\mu}=\frac{\sum_{i=1}^{n}x_{i}}{n}=\bar{x}
\end{align}

The MLE is the sample mean. Next we check the SOC


\begin{align}
\frac{\partial^2l(\theta|x)}{\partial \theta^2}=-n<0
\end{align}

We are in  a global maximum

\end{frame}
%----------------------------------------------------------------------%
\section{Conditional Likelihood Estimation}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Conditional Likelihood }

Suppose now, that $f(y,x|\eta)$ is the joint density function of two variables $X$ and $Y$.  Then, it can be decomposed as

\begin{align}
f(y,x|\eta) =f(y|x,\theta)f(x|\phi)
\end{align}

\begin{itemize}
  \item $\theta,\,\phi \subset \eta$ 
  \medskip
  \item The parameter vector of interest is  $\theta$
  \medskip
  \item Maximizing the joint likelihood is achieved through maximizing separately the conditional and the marginal likelihood
  \medskip
  \item The MLE of $\theta$ also maximizes the conditional likelihood  
  \medskip
  \item We can obtain ML estimates by specifying the conditional likelihood only
\end{itemize}  

 
\end{frame}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Example 1}

Let $y_i|X_i \sim_{iid} Bernoulli(p)$, where $p=Pr(y=1|X)=F(X\beta)$ and $F(.)$ normal cdf. Then the conditional likelihood is

\begin{align}
L(\beta,Y)=\Pi_{i=1}^n p_i^{y_i}(1-p_i)^{1-y_i}
\end{align}
The log likelihood is then

\begin{align}
l(\beta,Y)=\sum_{i=1}^n\left(y_i \ln F(X_i\beta)+(1-y_i)\ln(1-F(X_i\beta))\right)
\end{align}

\end{frame}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Example 1}
FOC

\begin{align}
\frac{\partial l(\beta|y,X)}{\partial\beta}&=0 
\end{align}
\begin{align}
\overset{n}{\underset{i=1}{\sum}}y_{i}\frac{1}{F(X_{i}\beta)}f(X_{i}'\beta)X_{i}'+\overset{n}{\underset{i=1}{\sum}}\left(1-y_{i}\right)\frac{1}{\left(1-F(x_{i}'\beta)\right)}-f(X_{i}'\beta)X_{i}'&=0
\end{align}

$\vdots$


\begin{align}
\sum_{i=1}^n\frac{(y_i-F(X_{i}'\beta))f(X_{i}'\beta)x_i}{F(X_{i}'\beta)(1-F(X_{i}'\beta))}=0
\end{align}

Note:
\begin{itemize}
  \item This is a system of $K$ non linear equations with $K$ unknown parameters. 
  \item We cannot explicitly solve for $\hat \beta$
\end{itemize}

\end{frame}

%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Example 2: Linear Regression}

Now consider the following linear model 
\medskip

\begin{align}
y=X\beta+u\,\,\,u\sim_{iid} N(0,\sigma^2I)
\end{align}

\medskip
Note that $y_i|X_i\sim N(X_i\beta,\sigma^2)$ thus the pdf of $y_i|X$
\medskip
\begin{align}
f_i(y_i|\beta,\sigma,X_i)=\frac{1}{(\sqrt{2\pi\sigma^2})}e^{-\frac{1}{2\sigma^2}(y_i-X_i\beta)^2}
\end{align}


\end{frame}

%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Example 2: Linear Regression}
The contribution to the log likelihood from observation $i$

\begin{align}
l_i(y_i|\beta,\sigma,X_i)=-\frac{1}{2}log 2 \pi-\frac{1}{2}log\sigma^2-\frac{1}{2\sigma^2}(y_i-X_i\beta)^2
\end{align}

Since we assumed that obs are $iid$, then the log likelihood

\begin{align}
l(y|\beta,\sigma,X) &=-\frac{n}{2}log 2 \pi-\frac{n}{2}log\sigma^2-\frac{1}{2\sigma^2}\sum_{i=1}^n(y_i-X_i\beta)^2  \\
&=-\frac{n}{2}log 2 \pi-\frac{n}{2}log\sigma^2-\frac{1}{2\sigma^2}(y-X\beta)'y-X\beta)
\end{align}
The ML estimators for $\beta$ and $\sigma$ result from maximizing this last line
\end{frame}


%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Example 2: Linear Regression}


The first step in maximizing $l(y|\beta,\sigma,X)$ is to {\bf concentrate} it with respect to $\sigma$

\begin{align}
\frac{\partial l}{\partial \sigma}&=-\frac{n}{2\sigma}-\frac{1}{\sigma^3}(y-X\beta)'y-X\beta)=0
\end{align}

Solving for $\sigma^2$

\begin{align}
\hat \sigma^2(\beta) = \frac{1}{n}(y-X\beta)'y-X\beta)
\end{align}

\end{frame}


%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Example 2: Linear Regression}

Replacing this in the log likelihood we get the concentrated (profile) likelihood

\begin{align}
l^c(y|\beta,X) = -\frac{n}{2}log 2 \pi-\frac{n}{2}log\left( \frac{1}{n}(y-X\beta)'y-X\beta)\right)-\frac{n}{2}
\end{align}

\medskip

\begin{enumerate}
 \item Get $\hat \beta$
 \item Replace $\beta$ in $\hat \sigma^2(\beta) = \frac{1}{n}(y-X\beta)'y-X\beta)$ $\rightarrow$ get $\hat \sigma^2$
\end{enumerate}

\medskip

This is not the only way, you could concentrate relative to $\beta$ first and solve for $\sigma^2$

\end{frame}

%----------------------------------------------------------------------%
\section{Bayesian Estimation}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Bayesian Estimation}

\begin{itemize}
\item The Bayesian approach to stats is fundamentally different from the classical approach we have been taking
\medskip
\item In the classical approach, the parameter $\theta$ is thought to be an unknown, but fixed quantity, e.g., $X_i\sim f(\theta)$
\medskip
\item In the Bayesian approach $\theta$ is considered to be a quantity whose variation can be described by a probability distribution  ({\it prior distribution})
\medskip
\item Then a sample is taken from a population indexed by $\theta$ and the prior is updated with this sample
\medskip
\item The resulting updated prior is the {\it posterior distribution}
\end{itemize}


\end{frame}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Bayesian Estimation}
For this updating we use {\it Bayes Theorem}

\bigskip
\begin{align}
\pi (\theta|X)=\frac{f(X|\theta)p(\theta)}{m(X)}
\end{align}

\bigskip
with $m(X)$ is the marginal distribution of $X$, i.e.

\begin{align}
m(X)=\int f(X|\theta)p(\theta)d\theta
\end{align}

\end{frame}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Bayesian Linear Regression}

Consider

\begin{align}
y_i= \beta x_i+u_i\,\,\,u_i \sim_{iid} N(0,\sigma^2I)
\end{align}

The likelihood function is

\begin{align}
f(y|\beta,\sigma,x)=\Pi_{i=1}^n\frac{1}{(\sqrt{2\pi\sigma^2})}e^{-\frac{1}{2\sigma^2}(y_i-\beta x_i)^2}
\end{align}
Now consider that the prior for $\beta$ is $N(\beta_0,\tau^2)$

\begin{align}
p(\beta)=\frac{1}{\sqrt{2\pi\tau^2}}e^{-\frac{1}{2\tau^2}{(\beta-\beta_0)^2}}
\end{align}

\end{frame}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Bayesian Linear Regression}
The Posterior distribution then 

\begin{align}
\pi (\beta|y,x) &= \frac{f(y,x|\beta)p(\beta)}{m(y,x)} \\
&= \frac{f(y|x,\beta)f(x|\beta)p(\beta)}{m(y,x)}
\end{align}

by assumption $f(x|\beta)=f(x)$

\begin{align}
&= f(y|x,\beta)p(\beta)\frac{f(x)}{m(y,x)}
\end{align}
\begin{align}
\propto f(y|x,\beta)p(\beta)
\end{align}

\end{frame}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Bayesian Linear Regression (Detour)}
{\bf Useful Result:} \\

\bigskip
Suppose a density of a random variable $\theta$ is proportional to

\begin{align}
  exp\left(\frac{-1}{2}(A\theta^2+B\theta)\right)
\end{align}

Then $\theta\sim N(m,V)$ where
\begin{align}
m=\frac{-1B}{2A} \,\,\,\,\,\, V=\frac{1}{A}
\end{align}


\end{frame}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Bayesian Linear Regression (we are back)}


\begin{align}
P(\beta|y,X) \propto \left(\frac{1}{\sqrt{2\pi\sigma^2}}\right)^n exp\left(\frac{-1}{2\sigma^2}\sum(y_i-\beta x_i)^2\right)exp\left(\frac{-1}{2\tau^2}(\beta-\beta_0)^2\right)
\end{align}


\begin{align}
\propto  exp \left[ \frac{-1}{2} \left(\frac{1}{\sigma^2}\sum(y_i-\beta x_i)^2 + \frac{-1}{\tau^2}(\beta-\beta_0)^2\right)\right]
\end{align}

\end{frame}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Bayesian Linear Regression (we are back)}

Using the previous detour

\begin{align}
A= \frac{1}{\sigma^2}\sum x_i^2 +\frac{1}{\tau^2}
\end{align}
\medskip
\begin{align}
B= -2 \frac{1}{\sigma^2}\sum y_i x_i +\frac{1}{\tau^2} \beta_0
\end{align}

Then $\beta\sim N(m,V)$ with

\begin{align}
m=\frac{\frac{1}{\sigma^2}\sum y_i x_i +\frac{1}{\tau^2} \beta_0}{(\frac{1}{\sigma^2}\sum x_i^2 +\frac{1}{\tau^2})}
\end{align}

\begin{align}
 V=\frac{1}{A}
\end{align}


\end{frame}

%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Bayesian Linear Regression (we are back)}

\begin{align}
m=\left(\frac{\frac{\sum x_i^2}{\sigma^2}}{\frac{\sum x_i^2}{\sigma^2}+\frac{1}{\tau^2}}\right)\frac{\sum x_iy_i}{\sum x_i^2} + \left(\frac{\frac{1}{\tau^2}}{\frac{\sum x_i^2}{\sigma^2}+\frac{1}{\tau^2}}\right)\beta_0
\end{align}

\medskip

\begin{align}
m = \omega \hat \beta_{MLE} + (1-\omega) \beta_0
\end{align}

Remarks 

\begin{itemize}
  \item If prior belief is strong $\tau \downarrow 0 \rightarrow \omega \downarrow 0 \implies m=\beta_0$ 
  \item If prior belief is weak $\tau \uparrow \infty \rightarrow \omega \uparrow 1 \implies m=\beta_{MLE}$ 

\end{itemize}

\end{frame}


%----------------------------------------------------------------------%
\begin{frame}
\frametitle{Review \& Next Steps}
  
  \begin{itemize} 
    \item Maximum Likelihood Estimation
    \medskip
    \item Conditional Maximum Likelihood Estimation
    \medskip
    \item Bayesian Estimation
  \bigskip  

  
  \item  {\bf Next Class:} Cont. Bayesian Stats.
  \bigskip
  \item Questions? Questions about software? 
  
  \end{itemize}


\end{frame}


%----------------------------------------------------------------------%

\section{Further Readings}
%----------------------------------------------------------------------%
\begin{frame}
\frametitle{Further Readings}
\footnotesize
\begin{itemize}
  \item Casella, G., \& Berger, R. L. (2002). Statistical inference (Vol. 2, pp. 337-472). Pacific Grove, CA: Duxbury.
  \medskip
   \item Davidson, R., \& MacKinnon, J. G. (2004). Econometric theory and methods (Vol. 5). New York: Oxford University Press.
  \medskip
  \item Efron, B., \& Hastie, T. (2016). Computer age statistical inference (Vol. 5). Cambridge University Press.
  \medskip
  \item Friedman, J., Hastie, T., \& Tibshirani, R. (2001). The elements of statistical learning (Vol. 1, No. 10). New York: Springer series in statistics.
  \medskip
  \item Hayashi, F. (2000). Econometrics. 2000. Princeton University Press. Section, 1, 60-69.
  
\end{itemize}

\end{frame}

%----------------------------------------------------------------------%

\end{document}

%----------------------------------------------------------------------%
%----------------------------------------------------------------------%

