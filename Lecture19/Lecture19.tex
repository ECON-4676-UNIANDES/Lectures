\documentclass[
  shownotes,
  xcolor={svgnames},
  hyperref={colorlinks,citecolor=DarkBlue,linkcolor=DarkRed,urlcolor=DarkBlue}
  , aspectratio=169]{beamer}
\usepackage{animate}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{pifont}
\usepackage{mathpazo}
%\usepackage{xcolor}
\usepackage{multimedia}
\usepackage{fancybox}
\usepackage[para]{threeparttable}
\usepackage{multirow}
\setcounter{MaxMatrixCols}{30}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{lscape}
\usepackage[compatibility=false,font=small]{caption}
\usepackage{booktabs}
\usepackage{ragged2e}
\usepackage{chronosys}
\usepackage{appendixnumberbeamer}
\usepackage{animate}
\setbeamertemplate{caption}[numbered]
\usepackage{color}
%\usepackage{times}
\usepackage{tikz}
\usepackage{comment} %to comment
%% BibTeX settings
\usepackage{natbib}
\bibliographystyle{apalike}
\bibpunct{(}{)}{,}{a}{,}{,}
\setbeamertemplate{bibliography item}{[\theenumiv]}

% Defines columns for bespoke tables
\usepackage{array}
\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}


\usepackage{xfrac}


\usepackage{multicol}
\setlength{\columnsep}{0.5cm}

% Theme and colors
\usetheme{Boadilla}

% I use steel blue and a custom color palette. This defines it.
\definecolor{andesred}{HTML}{af2433}

% Other options
\providecommand{\U}[1]{\protect\rule{.1in}{.1in}}
\usefonttheme{serif}
\setbeamertemplate{itemize items}[default]
\setbeamertemplate{enumerate items}[square]
\setbeamertemplate{section in toc}[circle]

\makeatletter

\definecolor{mybackground}{HTML}{82CAFA}
\definecolor{myforeground}{HTML}{0000A0}

\setbeamercolor{normal text}{fg=black,bg=white}
\setbeamercolor{alerted text}{fg=red}
\setbeamercolor{example text}{fg=black}

\setbeamercolor{background canvas}{fg=myforeground, bg=white}
\setbeamercolor{background}{fg=myforeground, bg=mybackground}

\setbeamercolor{palette primary}{fg=black, bg=gray!30!white}
\setbeamercolor{palette secondary}{fg=black, bg=gray!20!white}
\setbeamercolor{palette tertiary}{fg=white, bg=andesred}

\setbeamercolor{frametitle}{fg=andesred}
\setbeamercolor{title}{fg=andesred}
\setbeamercolor{block title}{fg=andesred}
\setbeamercolor{itemize item}{fg=andesred}
\setbeamercolor{itemize subitem}{fg=andesred}
\setbeamercolor{itemize subsubitem}{fg=andesred}
\setbeamercolor{enumerate item}{fg=andesred}
\setbeamercolor{item projected}{bg=gray!30!white,fg=andesred}
\setbeamercolor{enumerate subitem}{fg=andesred}
\setbeamercolor{section number projected}{bg=gray!30!white,fg=andesred}
\setbeamercolor{section in toc}{fg=andesred}
\setbeamercolor{caption name}{fg=andesred}
\setbeamercolor{button}{bg=gray!30!white,fg=andesred}


\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx}
\makeatletter

\definecolor{airforceblue}{rgb}{0.36, 0.54, 0.66}

\usepackage{tikz}
% Tikz settings optimized for causal graphs.
\usetikzlibrary{shapes,decorations,arrows,calc,arrows.meta,fit,positioning}
\tikzset{
    -Latex,auto,node distance =1 cm and 1 cm,semithick,
    state/.style ={ellipse, draw, minimum width = 0.7 cm},
    point/.style = {circle, draw, inner sep=0.04cm,fill,node contents={}},
    bidirected/.style={Latex-Latex,dashed},
    el/.style = {inner sep=2pt, align=left, sloped}
}


\makeatother






%%%%%%%%%%%%%%% BEGINS DOCUMENT %%%%%%%%%%%%%%%%%%

\begin{document}
 
\title[Lecture 19]{Lecture 19: \\ Classification (cont.)}
\subtitle{Big Data and Machine Learning for Applied Economics \\ Econ 4676}
\date{\today}

\author[Sarmiento-Barbieri]{Ignacio Sarmiento-Barbieri}
\institute[Uniandes]{Universidad de los Andes}


\begin{frame}[noframenumbering]
\maketitle
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%----------------------------------------------------------------------%
\begin{frame}
\frametitle{Announcements}

  
\begin{itemize} 
    \item {\bf Problem Set 1 is graded, I'll be uploading the graded version to Github} 
    \medskip
    \item  To help with the grading and improve organization, Jacob created a demo repo and a rubric. Please follow it!
    \medskip
    \item {\bf Problem Set 2 is due next Thursday September 22 at 11:00} 
    \medskip
    \item At some point this afternoon or tomorrow morning I'll send presentation assignments
    \medskip
    \item  You should consider class presentations as mini-seminars, just 2-5 minutes using one or two transparencies
    \medskip
    \item  Attempt to make a concise interpretation of the relevant material, making effective use of supporting numerical and graphical evidence.    
\end{itemize}
  
\end{frame}

%----------------------------------------------------------------------% 

\begin{frame}
\frametitle{Agenda}

\tableofcontents

\end{frame}

%----------------------------------------------------------------------%
\section{Recap }
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Classification: Motivation}

\begin{itemize}
\item Admit a student to $PEG$ based on their grades and LoR
\medskip
\item Give a credit, based on credit history, demographics?
\medskip
\item Classifying emails: spam, personal, social based on email contents
\medskip
\item Aim is to classify $y$ based on $X's$
\medskip
\item $y$ can be
\begin{itemize}
  \item qualitative (e.g., spam, personal, social)
  \item Not necessarily ordered
  \item Not necessarily two categories, but will start with the binary case

\end{itemize}
\end{itemize}


\end{frame}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Motivation}

\begin{itemize}
  \item Two states of nature $y \rightarrow i\in\{0,1\}$
  \medskip
  \item Two actions $(\hat{y}) \rightarrow j\in \{0,1\}$
\end{itemize}



        \begin{figure}[H] \centering
            \captionsetup{justification=centering}
              \includegraphics[scale=0.4]{figures/confusion_matrix}
              \\
              \tiny
              Source: \url{https://dzone.com/articles/understanding-the-confusion-matrix}
 \end{figure}

\end{frame}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Probability, Cost, and Classification}

\begin{itemize}
  \item Under a 0-1 penalty the problem boils down to finding $p=PR(Y=1|X)$
  \medskip
  \item We then predict 1 if $p>0.5$ and 0 otherwise (Bayes classifier)
  \medskip
  \item We can think 3 ways of finding this probability in binary cases
  \begin{itemize}
    \item Knn
    \item Logistic
    \item LDA
  \end{itemize}
\end{itemize}


\end{frame}
%----------------------------------------------------------------------%
\subsection{Logit}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Logit}
We have a conditional probability
\begin{align}
Pr(y=1|X) &= f(X'\beta) 
\end{align}

Logistic regression uses a $logit$ (sigmoid, softmax) link function

\begin{align}
p(y=1|X)=\frac{e^{X'\beta}}{1+e^{X'\beta}}=\frac{exp(\beta_0 +\beta_1 x_1 + \dots +\beta_k x_k)}{1+exp(\beta_0 +\beta_1 x_1 + \dots +\beta_k x_k)}
\end{align}

\end{frame}
%----------------------------------------------------------------------%
\subsection{Logit Demo}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Logit Demo}

\begin{scriptsize}
\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{101010}\NormalTok{) }\CommentTok{\#sets a seed }
\NormalTok{credit\textless{}{-}}\KeywordTok{readRDS}\NormalTok{(}\StringTok{"credit\_class.rds"}\NormalTok{)}
\CommentTok{\#70\% train}
\NormalTok{indic\textless{}{-}}\KeywordTok{sample}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\KeywordTok{nrow}\NormalTok{(credit),}\KeywordTok{floor}\NormalTok{(.}\DecValTok{7}\OperatorTok{*}\KeywordTok{nrow}\NormalTok{(credit)))}
\CommentTok{\#Partition the sample}
\NormalTok{train\textless{}{-}credit[indic,]}
\NormalTok{test\textless{}{-}credit[}\OperatorTok{{-}}\NormalTok{indic,]}
\KeywordTok{head}\NormalTok{(credit)}
\end{Highlighting}
\end{Shaded}
\end{scriptsize}
\begin{tiny}
\begin{verbatim}
##   Default duration amount installment age  history      purpose foreign  rent
## 1       0        6   1169           4  67 terrible goods/repair foreign FALSE
## 2       1       48   5951           2  22     poor goods/repair foreign FALSE
## 3       0       12   2096           2  49 terrible          edu foreign FALSE
## 4       0       42   7882           2  45     poor goods/repair foreign FALSE
## 5       1       24   4870           3  53     poor       newcar foreign FALSE
## 6       0       36   9055           2  35     poor          edu foreign FALSE
\end{verbatim}
\end{tiny}
\begin{scriptsize}


\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{dim}\NormalTok{(credit)}
\end{Highlighting}
\end{Shaded}
\end{scriptsize}
\begin{tiny}
\begin{verbatim}
## [1] 1000    9
\end{verbatim}
\end{tiny}


\end{frame}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Logit Demo}

\begin{scriptsize}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mylogit \textless{}{-}}\StringTok{ }\KeywordTok{glm}\NormalTok{(Default}\OperatorTok{\textasciitilde{}}\NormalTok{duration }\OperatorTok{+}\StringTok{ }\NormalTok{amount }\OperatorTok{+}\StringTok{ }\NormalTok{installment }\OperatorTok{+}\StringTok{ }\NormalTok{age }
              \OperatorTok{+}\StringTok{ }\KeywordTok{factor}\NormalTok{(history) }\OperatorTok{+}\StringTok{ }\KeywordTok{factor}\NormalTok{(purpose) }\OperatorTok{+}\StringTok{ }\KeywordTok{factor}\NormalTok{(foreign) }\OperatorTok{+}\StringTok{ }\KeywordTok{factor}\NormalTok{(rent), }
              \DataTypeTok{data =}\NormalTok{ train, }\DataTypeTok{family =} \StringTok{"binomial"}\NormalTok{)}
\KeywordTok{summary}\NormalTok{(mylogit)}
\end{Highlighting}
\end{Shaded}
\end{scriptsize}
\begin{tiny}
\begin{verbatim}
## 
## ...
## 
## Coefficients:
##                               Estimate Std. Error z value Pr(>|z|)    
## (Intercept)                 -3.285e-01  5.597e-01  -0.587 0.557264    
## duration                     1.625e-02  9.538e-03   1.704 0.088369 .  
## amount                       1.518e-04  4.325e-05   3.511 0.000447 ***
## installment                  3.335e-01  9.216e-02   3.619 0.000296 ***
## age                         -1.762e-02  8.851e-03  -1.990 0.046554 *  
## factor(history)poor         -1.212e+00  3.126e-01  -3.876 0.000106 ***
## factor(history)terrible     -1.989e+00  3.552e-01  -5.598 2.17e-08 ***
## factor(purpose)usedcar      -1.813e+00  4.067e-01  -4.459 8.23e-06 ***
## factor(purpose)goods/repair -7.163e-01  2.254e-01  -3.177 0.001486 ** 
## factor(purpose)edu           1.207e-01  3.858e-01   0.313 0.754450    
## factor(purpose)biz          -9.862e-01  3.440e-01  -2.867 0.004147 ** 
## factor(foreign)german       -2.057e+00  8.213e-01  -2.505 0.012254 *  
## factor(rent)TRUE             7.554e-01  2.355e-01   3.208 0.001337 ** 
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## ...
\end{verbatim}
\end{tiny}
\end{frame}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Logit Demo}

\begin{scriptsize}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{test}\OperatorTok{$}\NormalTok{phat\textless{}{-}}\StringTok{ }\KeywordTok{predict}\NormalTok{(mylogit, test, }\DataTypeTok{type=}\StringTok{"response"}\NormalTok{)}
\NormalTok{test}\OperatorTok{$}\NormalTok{Default\_hat\textless{}{-}}\KeywordTok{ifelse}\NormalTok{(test}\OperatorTok{$}\NormalTok{phat}\OperatorTok{\textgreater{}}\NormalTok{.}\DecValTok{5}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{)}
\KeywordTok{with}\NormalTok{(test,}\KeywordTok{prop.table}\NormalTok{(}\KeywordTok{table}\NormalTok{(Default,Default\_hat)))}
\end{Highlighting}
\end{Shaded}

\end{scriptsize}

\begin{columns}[T] % align columns
\begin{column}{.42\textwidth}
  

\begin{scriptsize}
\begin{verbatim}
##        Default_hat
## Default          0          1
##       0 0.63666667 0.06666667
##       1 0.22666667 0.07000000
\end{verbatim}
\end{scriptsize}

\end{column}  
\hfill
\begin{column}{.48\textwidth}

 \begin{figure}[H] \centering
            \captionsetup{justification=centering}
              \includegraphics[scale=0.3]{figures/confusion_matrix}                           
 \end{figure}

\end{column}
\end{columns}

\end{frame}
%----------------------------------------------------------------------%
\section{Linear Discriminant Analysis}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Linear Discriminant Analysis}


\centering
{\huge \textcolor{andesred}{Linear Discriminant Analysis}}


\end{frame}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Linear Discriminant Analysis}
\framesubtitle{Reverend Bayes to the rescue: Bayes Theorem}

\bigskip
\begin{align}
p (Y=1|X)=\frac{f(X|Y=1)p(Y=1)}{m(X)}
\end{align}

\bigskip
with $m(X)$ is the marginal distribution of $X$, i.e.

\begin{align}
m(X)=\int f(X|Y=1)p(Y=1)dy
\end{align}
Recall that there are two states of nature $y \rightarrow i\in\{0,1\}$
\begin{align}
m(X) &= f(X|Y=1)p(Y=1) + f(X|Y=0)p(Y=0) \nonumber \\
    &= f(X|Y=1)p(Y=1) + f(X|Y=0)(1-p(Y=1))
\end{align}

\end{frame}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Linear Discriminant Analysis}
\begin{itemize}
  \item This is basically an empirical Bayes approach
  \item We need to estimate $f(X|Y=1)$,  $f(X|Y=0)$ and $p(Y=1)$ 
  \begin{itemize}
    \item Let's start by estimating $p(Y=1)$. We've done this before
    \begin{align}
    p(Y=1) = \frac{\sum_{i=1}^n 1[Y_i=1]}{N}
    \end{align}
    \item Next $f(X|Y=j)$ with $j=0,1$. 
    \begin{itemize}
    \item if we assume one predictor and $X|Y\sim N(\mu_j,\sigma_j)$
    \item the problem boils down to estimating $\mu_j,\sigma_j$
    \item LDA makes it simpler, assumes $\sigma_j=\sigma$ $\forall j$
    \item then partition the sample in two $Y=0$ and $Y=1$, estimate the moments and get $\hat{f}(X|Y=j)$
    \end{itemize}
    \item Plug everything into the Bayes Rule and you're done
    

  \end{itemize}
\end{itemize}


\end{frame}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Linear Discriminant Analysis: Demo}
\begin{scriptsize}
\begin{align}
    p(Y=1) = \frac{\sum_{i=1}^n 1[Y_i=1]}{N}
  \end{align}


\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p1\textless{}{-}}\KeywordTok{sum}\NormalTok{(train}\OperatorTok{$}\NormalTok{Default)}\OperatorTok{/}\KeywordTok{dim}\NormalTok{(train)[}\DecValTok{1}\NormalTok{]}
\NormalTok{p1}
\end{Highlighting}
\end{Shaded}
\begin{verbatim}
## [1] 0.3014286
\end{verbatim}
\end{scriptsize}
\begin{scriptsize}
\begin{align}
\hat{\mu}_k=\frac{1}{n_k}\sum_{i:y_i=k}x_i
\end{align}


\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mu1\textless{}{-}}\KeywordTok{mean}\NormalTok{(train}\OperatorTok{$}\NormalTok{duration[train}\OperatorTok{$}\NormalTok{Default}\OperatorTok{==}\DecValTok{1}\NormalTok{])}
\NormalTok{mu1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 24.78673
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mu0\textless{}{-}}\KeywordTok{mean}\NormalTok{(train}\OperatorTok{$}\NormalTok{duration[train}\OperatorTok{$}\NormalTok{Default}\OperatorTok{==}\DecValTok{0}\NormalTok{])}
\NormalTok{mu0}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 19.79346
\end{verbatim}
\end{scriptsize}

\end{frame}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Linear Discriminant Analysis: Demo}


\begin{align}
\hat{\sigma}^2 = \frac{1}{N-K} \sum_{k=1}^K \sum_{i:y_i=k} (x_i -\hat{\mu}_k)^2
\end{align}

\begin{scriptsize}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{g1\textless{}{-}}\KeywordTok{sum}\NormalTok{((train}\OperatorTok{$}\NormalTok{duration[train}\OperatorTok{$}\NormalTok{Default}\OperatorTok{==}\DecValTok{1}\NormalTok{]}\OperatorTok{{-}}\NormalTok{mu1)}\OperatorTok{\^{}}\DecValTok{2}\NormalTok{)}
\NormalTok{g0\textless{}{-}}\KeywordTok{sum}\NormalTok{((train}\OperatorTok{$}\NormalTok{duration[train}\OperatorTok{$}\NormalTok{Default}\OperatorTok{==}\DecValTok{0}\NormalTok{]}\OperatorTok{{-}}\NormalTok{mu0)}\OperatorTok{\^{}}\DecValTok{2}\NormalTok{)}


\NormalTok{sigma\textless{}{-}}\KeywordTok{sqrt}\NormalTok{((g1}\OperatorTok{+}\NormalTok{g0)}\OperatorTok{/}\NormalTok{(}\KeywordTok{dim}\NormalTok{(train)[}\DecValTok{1}\NormalTok{]}\OperatorTok{{-}}\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}
\end{scriptsize}

\begin{align}
\hat{f}_k \sim N(\hat{\mu}_k,\hat{\sigma})
\end{align}

\begin{scriptsize}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{f1\textless{}{-}}\KeywordTok{dnorm}\NormalTok{(test}\OperatorTok{$}\NormalTok{duration,}\DataTypeTok{mean=}\NormalTok{mu1,}\DataTypeTok{sd=}\NormalTok{sigma)}
\NormalTok{f0\textless{}{-}}\KeywordTok{dnorm}\NormalTok{(test}\OperatorTok{$}\NormalTok{duration,}\DataTypeTok{mean=}\NormalTok{mu0,}\DataTypeTok{sd=}\NormalTok{sigma)}
\end{Highlighting}
\end{Shaded}
\end{scriptsize}

\end{frame}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Linear Discriminant Analysis: Demo}
\begin{scriptsize}


\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(}\StringTok{"MASS"}\NormalTok{)     }\CommentTok{\# LDA}
\NormalTok{lda\_simple \textless{}{-}}\StringTok{ }\KeywordTok{lda}\NormalTok{(Default}\OperatorTok{\textasciitilde{}}\NormalTok{duration, }\DataTypeTok{data =}\NormalTok{ train)}
\NormalTok{lda\_simple\_pred\textless{}{-}}\KeywordTok{predict}\NormalTok{(lda\_simple,test)}
\KeywordTok{names}\NormalTok{(lda\_simple\_pred)}
\end{Highlighting}
\end{Shaded}
\end{scriptsize}
\begin{tiny}
\begin{verbatim}
## [1] "class"     "posterior" "x"
\end{verbatim}
\end{tiny}


\begin{scriptsize}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{posteriors\textless{}{-}}\KeywordTok{data.frame}\NormalTok{(lda\_simple\_pred}\OperatorTok{$}\NormalTok{posterior)}
\NormalTok{posteriors}\OperatorTok{$}\NormalTok{hand\textless{}{-}f1}\OperatorTok{*}\NormalTok{p1}\OperatorTok{/}\NormalTok{(f1}\OperatorTok{*}\NormalTok{p1}\OperatorTok{+}\NormalTok{f0}\OperatorTok{*}\NormalTok{(}\DecValTok{1}\OperatorTok{{-}}\NormalTok{p1))}
\KeywordTok{head}\NormalTok{(posteriors)}
\end{Highlighting}
\end{Shaded}



\begin{verbatim}
##           X0        X1      hand
## 1  0.8013656 0.1986344 0.1986344
## 3  0.7668614 0.2331386 0.2331386
## 14 0.6861792 0.3138208 0.3138208
## 16 0.6861792 0.3138208 0.3138208
## 28 0.7668614 0.2331386 0.2331386
## 33 0.7283950 0.2716050 0.2716050
\end{verbatim}
\end{scriptsize}
\end{frame}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Linear Discriminant Analysis}
\framesubtitle{Extensions}

\begin{itemize}
    \item If we have $k$ predictors?
    \medskip
    \item then $X|Y~NM(\mu,\Sigma)$
    \begin{align}
    f(X|Y=j) = \frac{1}{(2\pi)^{k/2}|\Sigma|^{1/2}}exp(-\frac{1}{2}(x-\mu_j)'\Sigma_j(x-\mu_j)
    \end{align}
    \item $\mu_j$ is the vector of the sample means in each partition $j=0,1$
    \medskip
    \item $\Sigma_j$ is the matrix of variance and covariances of each partition $j=0,1$
    \medskip
    \item Can we lift normality? 
    \end{itemize}
\end{frame}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Linear Discriminant Analysis}

\begin{itemize}
  \item Why is it call linear?
  \item Note
  \begin{align}
    p>\frac{1}{2} \iff ln(\frac{p}{(1-p)})
  \end{align}
  \item Logit with one predictor
  \begin{align}
  \beta_1 + \beta_2 X
  \end{align}
  \item Classification: in the probability of space
  \item Discrimination: in the space of X
  \item $\beta_1 +\beta_2 X$ is the discrimination function for logit (it is lineal)
\end{itemize}

\end{frame}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Linear Discriminant Analysis}
\begin{itemize}
  \item LDA?
  \item One predictor with $\sigma_0 = \sigma_1$ (equal variance)
  \begin{align}
  p (Y=1|X)=\frac{f(X|Y=1)p(Y=1)}{f(X|Y=1)p(Y=1) + f(X|Y=0)(1-p(Y=1))}
  \end{align}
  \item Then under the equal variance assumption
  \begin{align}
  \frac{p (Y=1|X)}{1-p (Y=1|X} &= \frac{f(X|Y=1)p(Y=1)}{f(X|Y=0)(1-p(Y=1))} \\
                                &= \frac{p(Y=1)exp(\frac{-1}{2\sigma_1^2}(x-\mu_1)^2)}{(1-p(Y=1))exp(\frac{-1}{2\sigma_1^2}(x-\mu_0)^2)} 
  \end{align}
\end{itemize}

\end{frame}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Linear Discriminant Analysis}
\begin{itemize}
  \item Taking logs

\begin{align}
   log \left( \frac{p (Y=1|X)}{1-p (Y=1|X}\right)  &= log(\frac{p(Y=1)}{(1-p(Y=1))}-\frac{1}{2\sigma^2}(x-\mu_1)^2+\frac{1}{2\sigma^2}(x-\mu_0)^2 \\
  &= log(\frac{p(Y=1)}{(1-p(Y=1))}+\frac{1}{2\sigma^2}\left(\mu^2_0-\mu^2_1\right)+\frac{1}{\sigma^2}(\mu_1-\mu_0)x \\
  &= \gamma_1 +\gamma_2 X
  \end{align}  
  \item under the assumption of equal variance the discrimination function is lineal
  \item Note: logit estimates $\gamma_1$ and $\gamma_2$
\end{itemize}

\end{frame}
%----------------------------------------------------------------------%
\section{Misclassification Rates}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Misclassification Rates}


\centering
{\huge \textcolor{andesred}{Misclassification Rates}}


\end{frame}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Misclassification Rates}

\begin{itemize}
  \item Predicted probabilities from Logit model
\end{itemize}

\begin{figure}[H] \centering
            \captionsetup{justification=centering}
              \includegraphics[scale=0.6]{figures/box_plots}                            
 \end{figure}


\end{frame}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Misclassification Rates}




\begin{columns}[T] % align columns
\begin{column}{.52\textwidth}
\begin{itemize}
  \item A classification rule, or cutoff, is the probability $p$ at which you predict
  \medskip
  \begin{itemize}
    \item $\hat y_i =0$ if $p_i < p$
    \item $\hat y_i = 1$ if $p_i < p$
  \end{itemize}
  \medskip

  \item Measures of performance
  \begin{itemize}
  \item {\it 1-Specificity:} False Positive Rate, Type I error
  \item {\it Sensitivity:} True Positive Rate, power, (1-Type II error)
  \end{itemize}
  
\end{itemize}
\end{column}  
\hfill
\begin{column}{.48\textwidth}

 \begin{figure}[H] \centering
            \captionsetup{justification=centering}
              \includegraphics[scale=0.4]{figures/roc}                            
 \end{figure}

\end{column}
\end{columns}

\end{frame}
%----------------------------------------------------------------------%
\subsection{ROC curve}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{ROC}


\begin{itemize}
\item ROC curve: Receiver operating characteristic curve
\medskip
\item ROC curve illustrates the trade-off of the classification rule
\medskip
\item Gives us the ability
\begin{itemize}
  \item Measure the predictive capacity of our model
  \medskip
  \item Compare between models
  \medskip
\end{itemize}
  \item Some definitions
  \begin{itemize}
    \item $P=\sum Y_i$ positives
    \medskip
    \item $N=\sum(1-Y_i)$ negatives
    \medskip
    \item $T=P+N$ all observations
    \medskip
    \item True Positives: $TP=\sum \hat{Y}_i Y_i$, True Positive Rate = $\frac{TP}{P}$
    \medskip
    \item False Positives: $FP=\sum \hat{Y}_i (1-Y_i)$, False Positive Rate = $\frac{FP}{N}$

  \end{itemize}
\end{itemize}


\end{frame}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{ROC}

\begin{columns}[T] % align columns
\begin{column}{.52\textwidth}
  \begin{itemize}
    \item Binary Classifier: $\hat{Y}_i=1[p_i>c]$, $c\in[0,1]$
    \medskip
    \item Bayes fixes $c=0.5$
    \medskip
    \item Ideally $TPR=1$  and $FPR=0$
    \medskip
    \item ROC curve give us the locus of all possible $TPR$ and $FPR$ for all possible $c\in[0,1]$
  \end{itemize}
\end{column}  
\hfill
\begin{column}{.48\textwidth}

 \begin{figure}[H] \centering
            \captionsetup{justification=centering}
              \includegraphics[scale=0.4]{figures/roc}                            
 \end{figure}

\end{column}
\end{columns}

\end{frame}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{ROC}
\begin{itemize}
  \item ROC Properties 

  \begin{itemize}
  \item Has positive slope
  \begin{itemize}
    \item In $(0,0)$, $c=1$. When $c\downarrow$, $TP \uparrow$ and $FP\uparrow$. Then
    \begin{align}
    TPR = \sum \frac{\hat{Y}_iY_i}{P} \,\,\,\, FPR = \sum \frac{\hat{Y}_i(1-Y_i)}{T-P}
    \end{align}
    \item Is easy to show
    \begin{align}
    TPR &= \frac{\sum \hat{Y}_i}{P} - \frac{T-P}{P}FPR
    \end{align}
    \item ROC is the locus of all possible $TPR$ and $FPR$ for all possible $c\in[0,1]$ 
    \begin{align}
    TPR &= \frac{\sum \hat{Y}_i(c)}{P} - \frac{T-P}{P}FPR(c)
    \end{align}
  \end{itemize}
\end{itemize}  
\end{itemize}


\end{frame}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Inverse Classifier}
  \begin{itemize}
  \item ROC Properties 
  \begin{itemize}
    \item ROC curve is above the $45^{\circ}$ line (TPR=FPR)
    \item Note that
    \begin{align}
    \hat{Y}_i^F = 1- \hat{Y}_i 
    \end{align}
    \item Recall that
    \begin{align}
    TPR = \sum \frac{\hat{Y}_iY_i}{P} \,\,\,\, FPR = \sum \frac{\hat{Y}_i(1-Y_i)}{T-P}
    \end{align}
    \item the inverse clasifivier would be
    \begin{align}
    TPR^F = \sum \frac{(1-\hat{Y}_i)Y_i}{P} \,\,\,\, FPR^F = \sum \frac{(1-\hat{Y}_i)(1-Y_i)}{T-P}
    \end{align}
    \item Then $TPR - FPR= TPR^F - FPR^F$
    \item If ROC is bellow the $45^{\circ}$ line (TPR=FPR) then $FPR>TPR$. Given the above equality, the inverse classifier is above 
  \end{itemize}

\end{itemize}

\end{frame}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{ROC: Summary}

\begin{itemize}
  \item Ideal ROC curve
  \medskip
  \item AUC: area under the curve, is like an $R^2$
  \medskip
  \item Help us compare between classifiers
  \medskip
  \item Dominated classifiers?
  \medskip
  \item Which c? Choose a max $FPR$

\end{itemize}

\end{frame}
%----------------------------------------------------------------------%
\subsection{Roc Demo}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Roc Demo}
\begin{scriptsize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(}\StringTok{"ROCR"}\NormalTok{) }\CommentTok{\#Roc}

\NormalTok{mylogit \textless{}{-}}\StringTok{ }\KeywordTok{glm}\NormalTok{(Default}\OperatorTok{\textasciitilde{}}\NormalTok{duration }\OperatorTok{+}\StringTok{ }\NormalTok{amount }\OperatorTok{+}\StringTok{ }\NormalTok{installment }\OperatorTok{+}\StringTok{ }\NormalTok{age }
                \OperatorTok{+}\StringTok{ }\KeywordTok{factor}\NormalTok{(history) }\OperatorTok{+}\StringTok{ }\KeywordTok{factor}\NormalTok{(purpose) }\OperatorTok{+}\StringTok{ }\KeywordTok{factor}\NormalTok{(foreign) }\OperatorTok{+}\StringTok{ }\KeywordTok{factor}\NormalTok{(rent), }
                \DataTypeTok{data =}\NormalTok{ train, }\DataTypeTok{family =} \StringTok{"binomial"}\NormalTok{)}
\NormalTok{test}\OperatorTok{$}\NormalTok{phat\textless{}{-}}\StringTok{ }\KeywordTok{predict}\NormalTok{(mylogit, test, }\DataTypeTok{type=}\StringTok{"response"}\NormalTok{)}
\NormalTok{pred \textless{}{-}}\StringTok{ }\KeywordTok{prediction}\NormalTok{(test}\OperatorTok{$}\NormalTok{phat, test}\OperatorTok{$}\NormalTok{Default)}
\NormalTok{roc\_ROCR \textless{}{-}}\StringTok{ }\KeywordTok{performance}\NormalTok{(pred,}\StringTok{"tpr"}\NormalTok{,}\StringTok{"fpr"}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(roc\_ROCR, }\DataTypeTok{main =} \StringTok{"ROC curve"}\NormalTok{, }\DataTypeTok{colorize =}\NormalTok{ T)}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{a =} \DecValTok{0}\NormalTok{, }\DataTypeTok{b =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}
\end{scriptsize}


 \begin{figure}[H] \centering
            \captionsetup{justification=centering}
              \includegraphics[scale=0.4]{figures/unnamed-chunk-7-1.pdf}
 \end{figure}

\end{frame}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Roc Demo}


 \begin{figure}[H] \centering
            \captionsetup{justification=centering}
              \includegraphics[scale=0.4]{figures/unnamed-chunk-7-1.pdf}
 \end{figure}

\begin{scriptsize}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{auc\_ROCR \textless{}{-}}\StringTok{ }\KeywordTok{performance}\NormalTok{(pred, }\DataTypeTok{measure =} \StringTok{"auc"}\NormalTok{)}
\NormalTok{auc\_ROCR}\OperatorTok{@}\NormalTok{y.values[[}\DecValTok{1}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.714415
\end{verbatim}
\end{scriptsize}

\end{frame}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Roc Demo}

\begin{scriptsize}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mylda \textless{}{-}}\StringTok{ }\KeywordTok{lda}\NormalTok{(Default}\OperatorTok{\textasciitilde{}}\NormalTok{duration }\OperatorTok{+}\StringTok{ }\NormalTok{amount }\OperatorTok{+}\StringTok{ }\NormalTok{installment }\OperatorTok{+}\StringTok{ }\NormalTok{age , }\DataTypeTok{data =}\NormalTok{ train)}
\NormalTok{mylda}
\end{Highlighting}
\end{Shaded}
\end{scriptsize}

\begin{tiny}

\begin{verbatim}
## Call:
## lda(Default ~ duration + amount + installment + age, data = train)
## 
## Prior probabilities of groups:
##         0         1 
## 0.6985714 0.3014286 
## 
## Group means:
##   duration   amount installment      age
## 0 19.79346 3062.888    2.885481 36.40900
## 1 24.78673 4057.791    3.109005 33.85782
## 
## Coefficients of linear discriminants:
##                       LD1
## duration     0.0296041361
## amount       0.0002055164
## installment  0.4821242957
## age         -0.0386710882
\end{verbatim}
\end{tiny}

\end{frame}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Roc Demo}
\begin{scriptsize}


\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{phat\_mylda\textless{}{-}}\StringTok{ }\KeywordTok{predict}\NormalTok{(mylda, test, }\DataTypeTok{type=}\StringTok{"response"}\NormalTok{)}
\NormalTok{pred\_mylda \textless{}{-}}\StringTok{ }\KeywordTok{prediction}\NormalTok{(phat\_mylda}\OperatorTok{$}\NormalTok{posterior[,}\DecValTok{2}\NormalTok{], test}\OperatorTok{$}\NormalTok{Default)}

\NormalTok{roc\_mylda \textless{}{-}}\StringTok{ }\KeywordTok{performance}\NormalTok{(pred\_mylda,}\StringTok{"tpr"}\NormalTok{,}\StringTok{"fpr"}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(roc\_mylda, }\DataTypeTok{main =} \StringTok{"ROC curve"}\NormalTok{, }\DataTypeTok{colorize =}\NormalTok{ T)}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{a =} \DecValTok{0}\NormalTok{, }\DataTypeTok{b =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}


\begin{figure}[H] \centering
            \captionsetup{justification=centering}
              \includegraphics[scale=0.4]{figures/unnamed-chunk-10-1.pdf}
 \end{figure}
\end{scriptsize}
\end{frame}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Roc Demo}

\begin{scriptsize}
\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(roc\_ROCR, }\DataTypeTok{main =} \StringTok{"ROC curve"}\NormalTok{, }\DataTypeTok{colorize =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{col=}\StringTok{"red"}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(roc\_mylda,}\DataTypeTok{add=}\OtherTok{TRUE}\NormalTok{, }\DataTypeTok{colorize =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{col=}\StringTok{"blue"}\NormalTok{)}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{a =} \DecValTok{0}\NormalTok{, }\DataTypeTok{b =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}
\end{scriptsize}

\begin{figure}[H] \centering
            \captionsetup{justification=centering}
              \includegraphics[scale=0.4]{figures/unnamed-chunk-11-1.pdf}
 \end{figure}

\end{frame}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Roc Demo}
\begin{itemize}
  \item Area under the curve (AUC)

\end{itemize}
\begin{scriptsize}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{auc\_ROCR \textless{}{-}}\StringTok{ }\KeywordTok{performance}\NormalTok{(pred, }\DataTypeTok{measure =} \StringTok{"auc"}\NormalTok{)}
\NormalTok{auc\_ROCR\_lda\_simple \textless{}{-}}\StringTok{ }\KeywordTok{performance}\NormalTok{(pred\_mylda, }\DataTypeTok{measure =} \StringTok{"auc"}\NormalTok{)}
\NormalTok{auc\_ROCR}\OperatorTok{@}\NormalTok{y.values[[}\DecValTok{1}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.714415
\end{verbatim}


\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{auc\_ROCR\_lda\_simple}\OperatorTok{@}\NormalTok{y.values[[}\DecValTok{1}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}


\begin{verbatim}
## [1] 0.6291602
\end{verbatim}
\end{scriptsize}

\end{frame}

%----------------------------------------------------------------------%
\section{Review
 \& Next Steps}
%----------------------------------------------------------------------%
\begin{frame}
\frametitle{Review \& Next Steps}
  
\begin{itemize} 
    \item Review Classification:
    \medskip
    \begin{itemize} 
      \item KNN
        \begin{itemize}  
            \item Intuitive
            \item Not very useful in practice, curse of dimensionality
        \end{itemize}      
     \medskip   
    \item Logit
    \medskip
  \item Linear Discriminant Analysis
  \medskip
  \item  Misclassification Rates: ROC curve
  \medskip
  \item QDA?
  \medskip 
  \item Multiple Classes?

    \end{itemize}
    \bigskip  
  \item  Next class:  Problem Sets, Text Data!


\bigskip  
\item Questions? Questions about software? 

\end{itemize}
\end{frame}
%----------------------------------------------------------------------%
\section{Further Readings}
%----------------------------------------------------------------------%
\begin{frame}
\frametitle{Further Readings}

\begin{itemize}


  \item Friedman, J., Hastie, T., \& Tibshirani, R. (2001). The elements of statistical learning (Vol. 1, No. 10). New York: Springer series in statistics.
  \medskip
  \item James, G., Witten, D., Hastie, T., \& Tibshirani, R. (2013). An introduction to statistical learning (Vol. 112, p. 18). New York: springer.
  \medskip
  \item Taddy, M. (2019). Business data science: Combining machine learning and economics to optimize, automate, and accelerate business decisions. McGraw Hill Professional.
  
  
\end{itemize}

\end{frame}






%----------------------------------------------------------------------%
%----------------------------------------------------------------------%
\end{document}
%----------------------------------------------------------------------%
%----------------------------------------------------------------------%

